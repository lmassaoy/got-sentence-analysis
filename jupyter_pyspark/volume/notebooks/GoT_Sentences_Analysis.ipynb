{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.postimg.cc/vZ8k9N0K/Screen-Shot-2020-08-03-at-21-54-55.png](https://i.postimg.cc/vZ8k9N0K/Screen-Shot-2020-08-03-at-21-54-55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game of Thrones Sentences Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a great way to get some insights about the worldwide famous HBO tv serie '[Game of Thrones](https://en.wikipedia.org/wiki/Game_of_Thrones)', based on the novel '[A Song of Ice and Fire](https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire)' writen by George R. R. Martin.\n",
    "\n",
    "Also, you'll be able to see how to explore data using Pyspark, using some techniques to clean and enhance the dataset.\n",
    "\n",
    "The dataset was download from Kaggle, where you can find other amazing datasets published by the members.\n",
    "\n",
    "I hope you enjoy this study! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin starting the Spark Context (I'm using PySpark and Jupyter over a Docker container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we start creating a dataframe by reading the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+---------+----------------+------------+--------------------+\n",
      "|Release Date|  Season|  Episode|   Episode Title|        Name|            Sentence|\n",
      "+------------+--------+---------+----------------+------------+--------------------+\n",
      "|  2011-04-17|Season 1|Episode 1|Winter is Coming|waymar royce|What do you expec...|\n",
      "|  2011-04-17|Season 1|Episode 1|Winter is Coming|        will|I've never seen w...|\n",
      "|  2011-04-17|Season 1|Episode 1|Winter is Coming|waymar royce|How close did you...|\n",
      "|  2011-04-17|Season 1|Episode 1|Winter is Coming|        will|Close as any man ...|\n",
      "|  2011-04-17|Season 1|Episode 1|Winter is Coming|       gared|We should head ba...|\n",
      "+------------+--------+---------+----------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df = spark.read.csv(\"../datasets/raw_data/got/Game_of_Thrones_Script.csv\", header = True)\n",
    "sentences_df.createOrReplaceTempView(\"sentences\")\n",
    "\n",
    "sentences_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character and Houses\n",
    "### We're about to enhance the dataset with treatments over the characters' name column. Fixing wrong names and attaching last names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this example of the issue we may fix. The character 'Alliser Thorne' appears in the dataset with 5 different names we can identify by the resemblance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          Name|\n",
      "+--------------+\n",
      "|       alliser|\n",
      "|      allister|\n",
      "|alliser thorne|\n",
      "| alliser thorn|\n",
      "|alliser throne|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct Name from sentences where Name like 'alli%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we'd need a new dataset to join the wrong names with the right ones. But the problem is: it wasn't provided with the sentences dataset, so I needed to do it by myself :x please enjoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of the dataset to enhance characters' names and houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+---------+\n",
      "|CharacterName |VerifiedName   |House    |\n",
      "+--------------+---------------+---------+\n",
      "|A Voice       |A Voice        |null     |\n",
      "|Addam Marbrand|Addam Marbrand |Marbrand |\n",
      "|Aemon         |Aemon Targaryen|Targaryen|\n",
      "|Aeron         |Aeron Greyjoy  |Greyjoy  |\n",
      "|Aerson        |Aeron Greyjoy  |Greyjoy  |\n",
      "|Ahsa          |Asha Greyjoy   |Greyjoy  |\n",
      "|All           |All            |null     |\n",
      "|All Three     |All Three      |null     |\n",
      "|All Together  |All Together   |null     |\n",
      "|Alliser       |Alliser Thorne |Thorne   |\n",
      "+--------------+---------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characters_df = spark.read.csv(\"../datasets/raw_data/got/Characters_Dataset.csv\", header = True)\n",
    "characters_df.createOrReplaceTempView(\"characters\")\n",
    "characters_df.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first transformation applied to the dataset is:\n",
    "- Change the 'Season' column to be more comfortable to work with\n",
    "- Change the 'Episode' column to be more comfortable to work with\n",
    "- Create a new column: 'SeasonEpisode'\n",
    "- Applying camelcase over 'CharacterName' column\n",
    "- Create a new column: 'SentenceSize'\n",
    "- Merge with the Characters_Dataset to enhance names, changing 'CharacterName' column\n",
    "- Bring from Characters_Dataset 'House' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------+-------------+----------------+-------------+-----+--------------------+------------+\n",
      "|ReleaseDate|Season|Episode|SeasonEpisode|    EpisodeTitle|CharacterName|House|            Sentence|SentenceSize|\n",
      "+-----------+------+-------+-------------+----------------+-------------+-----+--------------------+------------+\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming| Waymar Royce| null|What do you expec...|         137|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|         Will| null|I've never seen w...|         103|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming| Waymar Royce| null|How close did you...|          22|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|         Will| null|Close as any man ...|          23|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|        Gared| null|We should head ba...|          32|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|   Yohn Royce| null|Do the dead frigh...|          25|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|        Gared| null|Our orders were t...|          87|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|   Yohn Royce| null|You don't think h...|          67|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|         Will| null|Whatever did it t...|          73|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|   Yohn Royce| null|It's a good thing...|         197|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|   Yohn Royce| null|Your dead men see...|          38|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|         Will| null|     They were here.|          15|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|        Gared| null|See where they went.|          20|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|   Yohn Royce| null|         What is it?|          11|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|        Gared| null|              It's …|           6|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|     Jon Snow|Stark|Go on. Father's w...|          25|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|     Jon Snow|Stark|    And your mother.|          16|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|Septa Mordane| null|Fine work, as alw...|          32|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|  Sansa Stark|Stark|          Thank you.|          10|\n",
      "| 2011-04-17|   S01|    E01|       S01E01|Winter is Coming|Septa Mordane| null|I love the detail...|          97|\n",
      "+-----------+------+-------+-------------+----------------+-------------+-----+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        select ReleaseDate,\n",
    "                Season,\n",
    "                Episode,\n",
    "                concat(Season,Episode) as SeasonEpisode,\n",
    "                EpisodeTitle,\n",
    "                VerifiedName as CharacterName,\n",
    "                House,\n",
    "                Sentence,\n",
    "                SentenceSize\n",
    "        from (\n",
    "            select `Release Date` as ReleaseDate,\n",
    "                    replace(Season,\"Season \",\"S0\") as Season,\n",
    "                    if(char_length(substring(Episode,instr(Episode,\" \")+1,3)) = 1,\n",
    "                        concat(\"E0\",substring(Episode,instr(Episode,\" \")+1,3)),\n",
    "                        concat(\"E\",substring(Episode,instr(Episode,\" \")+1,3)))\n",
    "                        as Episode,\n",
    "                    `Episode Title` as EpisodeTitle,\n",
    "                    initcap(Name) as CharacterName,\n",
    "                    Sentence,\n",
    "                    char_length(Sentence) as SentenceSize\n",
    "            from sentences            \n",
    "                    ) sentences\n",
    "        left join characters\n",
    "        on sentences.CharacterName = characters.CharacterName\n",
    "                    \n",
    "        \"\"\"\n",
    "sentences_df_refined = spark.sql(query)\n",
    "sentences_df_refined.createOrReplaceTempView(\"sentences_refined\")\n",
    "sentences_df_refined.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------- OPTIONAL!!! [OPEN] ---------------------------------------------------- \n",
    "### Maybe you wanna save this refined dataset as .parquet and use it instead of using .csv\n",
    "FYI: The original dataset in .csv is almost 3 times heavier than the .parquet (.parquet files are compressed - in our case the compression applied is named as 'snappy'). Also, you can save using the partitionBy commonly done in professional projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- It took 8.20419430732727 seconds to persist your dataset as parquet ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "sentences_df_refined.write.mode(\"overwrite\").partitionBy('Season','Episode').parquet(\"../datasets/refined_data/got/refined_dataset\")\n",
    "print(\"--- It took %s seconds to persist your dataset as parquet ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result will be like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: Season=S01\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E07\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E08\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E09\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E10\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "Directory: Season=S02\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E07\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E08\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E09\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E10\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "Directory: Season=S03\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E07\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E08\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E09\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E10\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "Directory: Season=S04\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E07\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E08\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E09\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E10\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "Directory: Season=S05\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E07\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E08\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E09\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E10\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "Directory: Season=S06\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E07\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E08\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E09\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E10\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "Directory: Season=S07\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E07\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "Directory: Season=S08\n",
      "\tDirectory: Episode=E01\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E02\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E03\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E04\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E05\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n",
      "\tDirectory: Episode=E06\n",
      "\t\tItem: .part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet.crc\n",
      "\t\tItem: part-00000-93a9c0ae-3e56-49fe-a74b-12ef01cbf050.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "path = \"../datasets/refined_data/got/refined_dataset/\"\n",
    "\n",
    "files = sorted(os.listdir(path))\n",
    "for f in files:\n",
    "    if os.path.isdir(path+f):\n",
    "        print(f\"Directory: {f}\")\n",
    "        episodes = sorted(os.listdir(path+f))\n",
    "        for episode in episodes:\n",
    "            if os.path.isdir(path+f+\"/\"+episode):\n",
    "                print(f\"\\tDirectory: {episode}\")\n",
    "                files = sorted(os.listdir(path+f+\"/\"+episode))\n",
    "                for file in files:\n",
    "                    print(f\"\\t\\tItem: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+---------------+---------+--------------------+------------+------+-------+\n",
      "|ReleaseDate|SeasonEpisode|EpisodeTitle|  CharacterName|    House|            Sentence|SentenceSize|Season|Episode|\n",
      "+-----------+-------------+------------+---------------+---------+--------------------+------------+------+-------+\n",
      "| 2017-08-13|       S07E05|   Eastwatch|Jaime Lannister|Lannister|You could have ki...|          25|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|What the fuck wer...|          40|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|Jaime Lannister|Lannister|Ending the war by...|          30|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|You saw the drago...|          39|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|looks incredulous...|          29|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|                And?|           4|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|Jaime Lannister|Lannister|sits up and says ...|          65|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|Listen to me, cun...|         130|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|Jaime Lannister|Lannister|That was only one...|          94|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|      You're fucked.|          14|   S07|    E05|\n",
      "+-----------+-------------+------------+---------------+---------+--------------------+------------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df_refined = spark.read.parquet(\"../datasets/refined_data/got/refined_dataset\")\n",
    "sentences_df_refined.createOrReplaceTempView(\"sentences_refined\")\n",
    "sentences_df_refined.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------------------------------- OPTIONAL!!! [CLOSED] ---------------------------------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "### The first thing you may ask when you see a dataset of sentences is Word Count. Basically we split the sentences word by word, then we count :)\n",
    "We assemble a couple functions and parameters to help us to handle the words found in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_replace_for_empty(word,character_list):\n",
    "    if len(character_list) == 1:\n",
    "        target = f\"replace({word},'{character_list[0]}','')\"\n",
    "        return target\n",
    "    else:\n",
    "        for character in character_list:\n",
    "            if character == character_list[0]:\n",
    "                target = f\"replace({word},'{character}','')\"\n",
    "            else:\n",
    "                target = f\"replace({target},'{character}','')\"\n",
    "        return target\n",
    "    \n",
    "\n",
    "def generate_if_then(word,if_list):\n",
    "    for if_case in if_list:\n",
    "        if if_case == if_list[0]:\n",
    "            target = f\"if(Word = '{if_case[0]}','{if_case[1]}',{word})\"\n",
    "        else:\n",
    "            target = f\"if(Word = '{if_case[0]}','{if_case[1]}',{target})\"\n",
    "    return target\n",
    "\n",
    "    \n",
    "character_list = ['.','[',']','?','/',',','(',')','!','…',\"|\",\"”\",\"‘\",\"\\\"\",'-','*','—',\"#\",'{','}','“','&',';','–','1','2','3','4','5','6','7','8','9','0',' ']\n",
    "if_list = [['re','are'],['t','not'],['d','would'],['ll','will'],['won','will']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to select only the column 'Sentence' from the dataset and start splitting the column based in some characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "|   You|\n",
      "| could|\n",
      "|  have|\n",
      "|killed|\n",
      "|   me.|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting by Spaces\n",
    "sentences_df_word_split = sentences_df_refined.select(split(sentences_df_refined.Sentence, '\\s+').alias('split'))\n",
    "\n",
    "sentences_df_word_single = sentences_df_word_split.select(explode(sentences_df_word_split.split).alias('Word'))\n",
    "\n",
    "# Splitting by Apostrophes\n",
    "sentences_df_word_split = sentences_df_word_single.select(split(sentences_df_word_single.Word, '\\'').alias('split'))\n",
    "\n",
    "sentences_df_word_single = sentences_df_word_split.select(explode(sentences_df_word_split.split).alias('Word'))\n",
    "\n",
    "# Splitting by …\n",
    "\n",
    "sentences_df_word_split = sentences_df_word_single.select(split(sentences_df_word_single.Word, '…').alias('split'))\n",
    "\n",
    "sentences_df_word_single = sentences_df_word_split.select(explode(sentences_df_word_split.split).alias('Word'))\n",
    "\n",
    "# Splitting by ;\n",
    "\n",
    "sentences_df_word_split = sentences_df_word_single.select(split(sentences_df_word_single.Word, ';').alias('split'))\n",
    "\n",
    "sentences_df_word_single = sentences_df_word_split.select(explode(sentences_df_word_split.split).alias('Word'))\n",
    "\n",
    "# Different of empty\n",
    "\n",
    "sentences_df_words = sentences_df_word_single.where(sentences_df_word_single.Word != '')\n",
    "sentences_df_words.createOrReplaceTempView(\"sentences_words\")\n",
    "\n",
    "sentences_df_words.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you noticed there are some special characters we need to get rid of, like commas, slashes, and others. We can also enhance the dataset inserting a column with the length of the word. On top of this, we can lowercase all words in order to achieve correct results when we group them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  Word|WordSize|\n",
      "+------+--------+\n",
      "|   you|       3|\n",
      "| could|       5|\n",
      "|  have|       4|\n",
      "|killed|       6|\n",
      "|    me|       2|\n",
      "+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanse_query = f\"\"\"\n",
    "        select Word,char_length(Word) as WordSize\n",
    "        from (\n",
    "            select rtrim(lower({generate_replace_for_empty('Word',character_list)})) as Word\n",
    "            from sentences_words\n",
    "            )\n",
    "        where char_length(Word) > 0\n",
    "        \"\"\"\n",
    "\n",
    "sentences_df_words_cleansed = spark.sql(cleanse_query)\n",
    "sentences_df_words_cleansed.createOrReplaceTempView(\"sentences_cleansed\")\n",
    "\n",
    "sentences_df_words_cleansed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the words dataset, there are some words we need to fix. Such as 'll' to 'will', 'd' to 'would', etc., generated by the word-split (for example: \"they're\" = 'they','re')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+\n",
      "|     Word|WordSize|WordCount|\n",
      "+---------+--------+---------+\n",
      "|     down|       4|      319|\n",
      "|     next|       4|       70|\n",
      "|     wits|       4|        4|\n",
      "|    drink|       5|      117|\n",
      "|  recruit|       7|        3|\n",
      "|   assume|       6|       20|\n",
      "|implicate|       9|        1|\n",
      "| infantry|       8|        3|\n",
      "|   kissed|       6|       10|\n",
      "|   locate|       6|        3|\n",
      "+---------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "short_fixing_query = f\"\"\"\n",
    "        select {generate_if_then('Word',if_list)} as Word,\n",
    "                WordSize,\n",
    "                count(Word) as WordCount\n",
    "        from sentences_cleansed\n",
    "        group by Word,WordSize\n",
    "        \"\"\"\n",
    "sentences_df_words_short_fixed = spark.sql(short_fixing_query)\n",
    "sentences_df_words_short_fixed.repartition(1).write.mode(\"overwrite\").parquet(\"../datasets/refined_data/got/word_count\")\n",
    "\n",
    "sentences_df_words_short_fixed.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're ready to get some insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df_words_short_fixed_parquet = spark.read.parquet(\"../datasets/refined_data/got/word_count\")\n",
    "sentences_df_words_short_fixed_parquet.cache()\n",
    "sentences_df_words_short_fixed_parquet.createOrReplaceTempView(\"word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top 10 words spoken in the tv series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Word|WordCount|\n",
      "+----+---------+\n",
      "| you|    12452|\n",
      "| the|    12228|\n",
      "|   i|    10054|\n",
      "|  to|     7972|\n",
      "|   a|     6093|\n",
      "| and|     5272|\n",
      "|   s|     4656|\n",
      "|  of|     4556|\n",
      "|  it|     3991|\n",
      "| not|     3788|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Word,WordCount from word_count order by WordCount desc limit 10;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top 10 longest words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|Word                 |WordSize|\n",
      "+---------------------+--------+\n",
      "|granddaughterwwwertha|21      |\n",
      "|somethingsomething   |18      |\n",
      "|seventysevencourse   |18      |\n",
      "|eastwatchbythesea    |17      |\n",
      "|kingbeyondthewall    |17      |\n",
      "|greatgrandfather     |16      |\n",
      "|responsibilities     |16      |\n",
      "|misunderstanding     |16      |\n",
      "|accomplishments      |15      |\n",
      "|tralalalaleeday      |15      |\n",
      "+---------------------+--------+\n",
      "\n",
      "as you can, see there are some issues with some words because of the absence of spaces :( anyway, one more thing to be aware in a future enhancement\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Word,WordSize from word_count order by WordSize desc limit 10;\").show(10,False)\n",
    "print(\"as you can, see there are some issues with some words because of the absence of spaces :( anyway, one more thing to be aware in a future enhancement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many times were said 'Jon' and 'Snow'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Word|WordCount|\n",
      "+----+---------+\n",
      "| jon|      284|\n",
      "|snow|      192|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Word,WordCount from word_count where Word = 'jon' or Word = 'snow' order by 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have you asked yourself how many times you heard the famous \"winter is coming\" while watched the tv show? Well, probably you thought you heard more than you actually did :O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.postimg.cc/CKbsgMJ9/Screen-Shot-2020-08-02-at-23-49-44.png](https://i.postimg.cc/CKbsgMJ9/Screen-Shot-2020-08-02-at-23-49-44.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times the sentence was found: 12\n",
      "--------------------------------------------\n",
      "at S01E01, Eddard \"Ned\" Stark says: He won't be a boy forever. And winter is coming.\n",
      "\n",
      "at S01E01, Benjen Stark says: Maybe. Direwolves south of the wall. Talk of the Walkers. My brother might be the next Hand to the king. Winter is coming.\n",
      "\n",
      "at S01E01, Eddard \"Ned\" Stark says: Winter is coming.\n",
      "\n",
      "at S01E03, Arya Stark says: Winter is coming.\n",
      "\n",
      "at S01E08, Robb Stark says: Tell Lord Tywin, Winter is coming for him.\n",
      "\n",
      "at S01E10, Yoren says: Come on, you sorry sons of whores! lt's a thousand leagues from here to the Wall, and winter is coming!\n",
      "\n",
      "at S02E03, Catelyn Stark says: Because it won't last. Because they are the knights of summer and winter is coming.\n",
      "\n",
      "at S03E03, Ramsay Bolton says: You're a long way from home and winter is coming.\n",
      "\n",
      "at S04E10, Mance Rayder says: I showed you everything I had. The whole army, a hundred thousand strong, and what did you do? You fired on us with everything you had. It wasn't much. Soon as I saw that I sent four hundred men to climb the wall. An unmanned stretch five miles west of here. A lot of them will die climbing, but most of them will be over by the end of the day. This is me being honest with you, Jon Snow, which is more than you've ever done for me. My people have bled enough. We're not here to conquer, we're here to hide behind your wall just like you. We need your tunnel. Now we both know that winter is coming and if my people aren't south of the wall when it comes in earnest, we'll all end up worse than dead. Wanna strike a bargain with me? Here's a bargain. You go back, you open the gates to us and I swear to you that no one else will die. Refuse and we'll kill every last man in Castle Black.\n",
      "\n",
      "at S05E03, Jon Snow says: You saved us from Mance Rayder's army. We will never forget that. But it's a question of survival. The Night's Watch can't continue to feed your men and the wildling prisoners indefinitely. Winter is coming.\n",
      "\n",
      "at S05E05, Jon Snow says: I know what it's like to lose the people you love. I know this is hard for you. But winter is coming. We know what's coming with it. We can't face it alone.\n",
      "\n",
      "at S05E07, Stannis Baratheon says: Winter is coming. Those arent just the Stark words, it's a fact. If we march back to Castle Black, we winter at Castle Black. And who can say how many years this winter will last.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_query = \"\"\"\n",
    "    select SeasonEpisode,\n",
    "            CharacterName,\n",
    "            Sentence from sentences_refined\n",
    "    where lower(Sentence) like '%winter is coming%'\n",
    "    order by SeasonEpisode\n",
    "    \"\"\"\n",
    "winter_is_coming = spark.sql(sentence_query)\n",
    "rows = winter_is_coming.collect()\n",
    "\n",
    "print(\"Number of times the sentence was found: \"+str(len(rows))+\"\\n--------------------------------------------\")\n",
    "\n",
    "for row in rows:\n",
    "    print(\"at \"+row[0]+\", \"+row[1]+\" says: \"+row[2]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This one is DEEP: \"The night is dark and full of terrors\". And surprisingly it's said more than the previous one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.postimg.cc/Y9qRzKfV/Screen-Shot-2020-08-02-at-23-55-36.png](https://i.postimg.cc/Y9qRzKfV/Screen-Shot-2020-08-02-at-23-55-36.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times the sentence was found: 18\n",
      "--------------------------------------------\n",
      "at S02E01, Melisandre says: Take them and cast your light upon us. For the night is dark and full of terrors.\n",
      "\n",
      "at S02E01, Group says: For the night is dark and full of terrors.\n",
      "\n",
      "at S02E01, Melisandre says: For the night is dark and full of terrors.\n",
      "\n",
      "at S02E01, Melisandre says: The night is dark and full of terrors, old man, but the fire burns them all away. Your Grace.\n",
      "\n",
      "at S02E04, Melisandre says: Look to your sins, Lord Renly. The night is dark and full of terrors.\n",
      "\n",
      "at S02E04, Davos Seaworth says: Someone once told me the night is dark and full of terrors.\n",
      "\n",
      "at S03E05, Thoros of Myr says: Show us the truth. Strike this man down if he is guilty. Give strength to his sword if he is true. Lord of Light, give us wisdom. For the night is dark and full of terrors.\n",
      "\n",
      "at S03E05, Men says: For the night is dark and full of terrors.\n",
      "\n",
      "at S03E05, Thoros of Myr says: For the night is dark and full of terrors. Lord, cast-\n",
      "\n",
      "at S03E05, Selyse Florent says: Lord, cast your light upon me. Protect me in the darkness. Burn away my sins. Help me serve you. Use me as you will. For the night is dark and full of terrors.\n",
      "\n",
      "at S04E02, Melisandre says: Lord of Light, protect us, for the night is dark and full of terrors.\n",
      "\n",
      "at S05E03, Priestess says: For the night is dark and full of terrors.\n",
      "\n",
      "at S05E03, Crowd says: For the night is dark and full of terrors.\n",
      "\n",
      "at S05E09, Melisandre says: Lord of Light protect us, for the night is dark and full of terrors!\n",
      "\n",
      "at S06E01, Listeners says: For the night is dark and full of terrors.\n",
      "\n",
      "at S06E07, Lem says: Stay safe. The night is dark and full of terrors.\n",
      "\n",
      "at S08E03, Melisandre says: Lord of Light, cast your light upon us. Lord of Light, defend us. For the night is dark and full of terrors!\n",
      "\n",
      "at S08E03, Melisandre says: Lord of Light, cast your light upon us! Lord of Light, defend us! For the night is dark and full of terrors!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_query = \"\"\"\n",
    "    select SeasonEpisode,\n",
    "            CharacterName,\n",
    "            Sentence from sentences_refined\n",
    "    where lower(Sentence) like '%the night is dark and full of terrors%'\n",
    "    order by SeasonEpisode\n",
    "    \"\"\"\n",
    "night_is_dark = spark.sql(sentence_query)\n",
    "rows = night_is_dark.collect()\n",
    "\n",
    "print(\"Number of times the sentence was found: \"+str(len(rows))+\"\\n--------------------------------------------\")\n",
    "\n",
    "for row in rows:\n",
    "    print(\"at \"+row[0]+\", \"+row[1]+\" says: \"+row[2]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may be wondering how'd be the ranking of the more talkative houses.\n",
    "### Let's take a look considering the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|    House|NumberOfSentences|\n",
      "+---------+-----------------+\n",
      "|Lannister|             4621|\n",
      "|    Stark|             4236|\n",
      "|Targaryen|             1178|\n",
      "|Baratheon|              789|\n",
      "|  Greyjoy|              744|\n",
      "+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_house_q = \"\"\"\n",
    "        select House,\n",
    "                count(Sentence) as NumberOfSentences\n",
    "        from sentences_refined\n",
    "        where House is not null\n",
    "        group by House\n",
    "        order by count(Sentence) desc\n",
    "        \"\"\"\n",
    "spark.sql(sentence_house_q).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and now considering the length of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|    House|TotalSentencesSize|\n",
      "+---------+------------------+\n",
      "|Lannister|            331012|\n",
      "|    Stark|            216493|\n",
      "|Targaryen|             74298|\n",
      "|Baratheon|             50376|\n",
      "|  Greyjoy|             46451|\n",
      "+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_house_q = \"\"\"\n",
    "        select House,\n",
    "                sum(SentenceSize) as TotalSentencesSize\n",
    "        from sentences_refined\n",
    "        where House is not null\n",
    "        group by House\n",
    "        order by sum(SentenceSize) desc\n",
    "        \"\"\"\n",
    "spark.sql(sentence_house_q).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I had the feeling the Lannisters would win this contest (maybe because the Starks are almost dead lol)\n",
    "![https://64.media.tumblr.com/072498d6f25b8b9c05577620876208c6/tumblr_nuymvbSm7X1ueoasuo1_500.gif](https://64.media.tumblr.com/072498d6f25b8b9c05577620876208c6/tumblr_nuymvbSm7X1ueoasuo1_500.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the episodes with more dialogues and fewer dialogues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+----------------+\n",
      "|SeasonEpisode|EpisodeTitle|CountSentences|SumSentencesSize|\n",
      "+-------------+------------+--------------+----------------+\n",
      "|S07E05       |Eastwatch   |505           |33866           |\n",
      "+-------------+------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogues_query = \"\"\"\n",
    "    select SeasonEpisode,\n",
    "    EpisodeTitle,\n",
    "    count(Sentence) as CountSentences,\n",
    "    sum(SentenceSize) as SumSentencesSize\n",
    "    from sentences_refined\n",
    "    group by SeasonEpisode,EpisodeTitle\n",
    "    order by count(Sentence) desc\n",
    "    limit 1;\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(dialogues_query).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------+--------------+----------------+\n",
      "|SeasonEpisode|EpisodeTitle          |CountSentences|SumSentencesSize|\n",
      "+-------------+----------------------+--------------+----------------+\n",
      "|S08E04       |The Last of the Starks|51            |2692            |\n",
      "+-------------+----------------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogues_query = \"\"\"\n",
    "    select SeasonEpisode,\n",
    "    EpisodeTitle,\n",
    "    count(Sentence) as CountSentences,\n",
    "    sum(SentenceSize) as SumSentencesSize\n",
    "    from sentences_refined\n",
    "    group by SeasonEpisode,EpisodeTitle\n",
    "    order by count(Sentence) asc\n",
    "    limit 1;\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(dialogues_query).show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of sentences and size of sentences per Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|Season|CountSentences|SumSentencesSize|\n",
      "+------+--------------+----------------+\n",
      "|   S01|          3179|          191173|\n",
      "|   S02|          3914|          239487|\n",
      "|   S03|          3573|          215795|\n",
      "|   S04|          3446|          212372|\n",
      "|   S05|          3035|          194068|\n",
      "|   S06|          2856|          185802|\n",
      "|   S07|          2442|          168716|\n",
      "|   S08|          1466|           68946|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogues_query = \"\"\"\n",
    "    select Season,\n",
    "    count(Sentence) as CountSentences,\n",
    "    sum(SentenceSize) as SumSentencesSize\n",
    "    from sentences_refined\n",
    "    group by Season\n",
    "    order by Season asc\n",
    "    \"\"\"\n",
    "\n",
    "spark.sql(dialogues_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "### Here's something interesting to take a look, gathering insights to apply in real professional projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.postimg.cc/Mp7WdNwm/sentiment-analysis.png](https://i.postimg.cc/Mp7WdNwm/sentiment-analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purpose here we'll use two different Python libs which can help us calculating the sentiment of a given text. You can deep dive into machine and deep learning if you prefer, but for our study, these two should do the trick ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the libs, initializing some objects and creating some helpful functions (which we'll use as UDF (user defined function) in SparkSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def get_textblob_score(sentence):\n",
    "    analysis = TextBlob(sentence)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "\n",
    "def get_vader_score(sentence):\n",
    "    return analyzer.polarity_scores(sentence)['compound']\n",
    "\n",
    "\n",
    "spark.udf.register(\"sentimentTextBlob\", get_textblob_score, DoubleType())\n",
    "spark.udf.register(\"sentimentVader\", get_vader_score, DoubleType())\n",
    "\n",
    "\n",
    "def generate_sentiment_face(score_column):\n",
    "    return f\"\"\"if({score_column}=0,\":|\",\n",
    "                    if({score_column}>0 and {score_column}<0.4,\":)\",\n",
    "                        if({score_column}>=0.4,\":D\",\n",
    "                            if({score_column}<0 and {score_column}>=-0.4,\":(\",\":'(\"))))\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def generate_sentiment_result(score_column):\n",
    "    return f\"\"\"if({score_column}=0,\"Neutral\",\n",
    "                    if({score_column}>0 and {score_column}<0.4,\"Positive\",\n",
    "                        if({score_column}>=0.4,\"Very Positive\",\n",
    "                            if({score_column}<0 and {score_column}>=-0.4,\"Negative\",\"Very Negative\"))))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a sample of the results we can get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+--------------------+----------------+----------------+\n",
      "|SeasonEpisode|     CharacterName|            Sentence|SentimentResult2|SentimentResult2|\n",
      "+-------------+------------------+--------------------+----------------+----------------+\n",
      "|       S07E05|   Jaime Lannister|You could have ki...|   Very Negative|   Very Negative|\n",
      "|       S07E05|             Bronn|What the fuck wer...|   Very Negative|   Very Negative|\n",
      "|       S07E05|   Jaime Lannister|Ending the war by...|   Very Negative|   Very Negative|\n",
      "|       S07E05|             Bronn|You saw the drago...|         Neutral|         Neutral|\n",
      "|       S07E05|             Bronn|looks incredulous...|         Neutral|         Neutral|\n",
      "|       S07E05|             Bronn|                And?|         Neutral|         Neutral|\n",
      "|       S07E05|   Jaime Lannister|sits up and says ...|         Neutral|         Neutral|\n",
      "|       S07E05|             Bronn|Listen to me, cun...|        Negative|        Negative|\n",
      "|       S07E05|   Jaime Lannister|That was only one...|         Neutral|         Neutral|\n",
      "|       S07E05|             Bronn|      You're fucked.|   Very Negative|   Very Negative|\n",
      "|       S07E05|   Jaime Lannister|Don't you mean we...|   Very Negative|   Very Negative|\n",
      "|       S07E05|             Bronn|No, I do not. Dra...|        Negative|        Negative|\n",
      "|       S07E05|   Jaime Lannister|I have to tell Ce...|         Neutral|         Neutral|\n",
      "|       S07E05|             Bronn|pauses and looks ...|         Neutral|         Neutral|\n",
      "|       S07E05|             Bronn|Best well jump ba...|   Very Positive|   Very Positive|\n",
      "|       S07E05|             Bronn|stands up and wal...|   Very Negative|   Very Negative|\n",
      "|       S07E05|  Tyrion Lannister|is walking over a...|   Very Negative|   Very Negative|\n",
      "|       S07E05|Daenerys Targaryen|I know what Cerse...|   Very Negative|   Very Negative|\n",
      "|       S07E05|Daenerys Targaryen|burn down your ho...|   Very Negative|   Very Negative|\n",
      "|       S07E05|Daenerys Targaryen|Step forward, My ...|         Neutral|         Neutral|\n",
      "+-------------+------------------+--------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_query = f\"\"\"\n",
    "    select SeasonEpisode,\n",
    "                CharacterName,\n",
    "                Sentence,\n",
    "                SentimentScore1,\n",
    "                {generate_sentiment_face('SentimentScore1')} as Sentiment1,\n",
    "                {generate_sentiment_result('SentimentScore1')} as SentimentResult1,\n",
    "                SentimentScore2,\n",
    "                {generate_sentiment_face('SentimentScore2')} as Sentiment2,\n",
    "                {generate_sentiment_result('SentimentScore2')} as SentimentResult2\n",
    "    from (\n",
    "        select SeasonEpisode,\n",
    "                CharacterName,\n",
    "                Sentence,\n",
    "                sentimentTextBlob(Sentence) as SentimentScore1,\n",
    "                sentimentVader(Sentence) as SentimentScore2\n",
    "        from sentences_refined\n",
    "        limit 20\n",
    "        )\n",
    "\"\"\"\n",
    "sample_sentences_df = spark.sql(sample_query)\n",
    "sample_sentences_df.select(\"SeasonEpisode\",\"CharacterName\",\"Sentence\",\"SentimentResult2\",\"SentimentResult2\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personally I enjoyed the results I got from VaderSentiment :) so I rather continue this journey trusting in it to be my main tool to do the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- It took 35.136321783065796 seconds to persist your dataset as parquet ---\n"
     ]
    }
   ],
   "source": [
    "sentiment_query = f\"\"\"\n",
    "    select *,\n",
    "            {generate_sentiment_result('SentimentScore')} as SentimentResult,\n",
    "            {generate_sentiment_face('SentimentScore')} as Sentiment\n",
    "    from (\n",
    "        select *,\n",
    "                sentimentVader(Sentence) as SentimentScore\n",
    "        from sentences_refined\n",
    "        )\n",
    "\"\"\"\n",
    "sentiment_df = spark.sql(sentiment_query)\n",
    "\n",
    "start_time = time.time()\n",
    "sentiment_df.write.mode(\"overwrite\").partitionBy('Season','Episode').parquet(\"../datasets/refined_data/got/sentiment_analysis\")\n",
    "print(\"--- It took %s seconds to persist your dataset as parquet ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+---------------+---------+--------------------+------------+--------------+---------------+---------+------+-------+\n",
      "|ReleaseDate|SeasonEpisode|EpisodeTitle|  CharacterName|    House|            Sentence|SentenceSize|SentimentScore|SentimentResult|Sentiment|Season|Episode|\n",
      "+-----------+-------------+------------+---------------+---------+--------------------+------------+--------------+---------------+---------+------+-------+\n",
      "| 2017-08-13|       S07E05|   Eastwatch|Jaime Lannister|Lannister|You could have ki...|          25|       -0.6705|  Very Negative|      :'(|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|What the fuck wer...|          40|       -0.5423|  Very Negative|      :'(|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|Jaime Lannister|Lannister|Ending the war by...|          30|       -0.8519|  Very Negative|      :'(|   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|You saw the drago...|          39|           0.0|        Neutral|       :||   S07|    E05|\n",
      "| 2017-08-13|       S07E05|   Eastwatch|          Bronn|     null|looks incredulous...|          29|           0.0|        Neutral|       :||   S07|    E05|\n",
      "+-----------+-------------+------------+---------------+---------+--------------------+------------+--------------+---------------+---------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df = spark.read.parquet(\"../datasets/refined_data/got/sentiment_analysis\")\n",
    "sentiment_df.createOrReplaceTempView(\"sentiment_analysis\")\n",
    "sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're reading from a new refined dataset and for our first exploration, let's see the distribution of sentiments around the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+-------+--------+------------+\n",
      "|Season|VeryNegative|Negative|Neutral|Positive|VeryPositive|\n",
      "+------+------------+--------+-------+--------+------------+\n",
      "|   S01|         439|     322|   1371|     365|         682|\n",
      "|   S02|         603|     416|   1689|     434|         772|\n",
      "|   S03|         536|     357|   1596|     411|         673|\n",
      "|   S04|         542|     329|   1505|     386|         684|\n",
      "|   S05|         447|     308|   1278|     356|         646|\n",
      "|   S06|         453|     320|   1232|     331|         520|\n",
      "|   S07|         395|     267|   1115|     252|         413|\n",
      "|   S08|         181|     137|    741|     159|         248|\n",
      "+------+------------+--------+-------+--------+------------+\n",
      "\n",
      "+------------+--------+-------+--------+------------+\n",
      "|VeryNegative|Negative|Neutral|Positive|VeryPositive|\n",
      "+------------+--------+-------+--------+------------+\n",
      "|        3596|    2456|  10527|    2694|        4638|\n",
      "+------------+--------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_split_query = \"\"\"\n",
    "    select Season,\n",
    "            sum(VeryNegative) as VeryNegative,\n",
    "            sum(Negative) as Negative,\n",
    "            sum(Neutral) as Neutral,\n",
    "            sum(Positive) as Positive,\n",
    "            sum(VeryPositive) as VeryPositive\n",
    "    from (\n",
    "        select Season,\n",
    "                if(SentimentResult='Very Negative',1,0) as VeryNegative,\n",
    "                if(SentimentResult='Negative',1,0) as Negative,\n",
    "                if(SentimentResult='Neutral',1,0) as Neutral,\n",
    "                if(SentimentResult='Positive',1,0) as Positive,\n",
    "                if(SentimentResult='Very Positive',1,0) as VeryPositive\n",
    "        from sentiment_analysis\n",
    "        )\n",
    "    group by Season\n",
    "    order by Season\n",
    "\"\"\"\n",
    "\n",
    "sentiment_distribution = spark.sql(sentiment_split_query)\n",
    "sentiment_distribution.cache()\n",
    "sentiment_distribution.show(10)\n",
    "sentiment_distribution.groupBy().agg(\\\n",
    "                                    sum(\"VeryNegative\").alias(\"VeryNegative\"),\\\n",
    "                                    sum(\"Negative\").alias(\"Negative\"),\\\n",
    "                                    sum(\"Neutral\").alias(\"Neutral\"),\\\n",
    "                                    sum(\"Positive\").alias(\"Positive\"),\\\n",
    "                                    sum(\"VeryPositive\").alias(\"VeryPositive\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Seasons with the highest and lowest count of each of the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(max(VeryNegative)=603, min(VeryNegative)=181, max(Negative)=416, min(Negative)=137, max(Neutral)=1689, min(Neutral)=741, max(Positive)=434, min(Positive)=159, max(VeryPositive)=772, min(VeryPositive)=248)\n",
      "\n",
      "\n",
      "+------+---------------+\n",
      "|Season|MaxVeryNegative|\n",
      "+------+---------------+\n",
      "|   S02|            603|\n",
      "+------+---------------+\n",
      "\n",
      "+------+---------------+\n",
      "|Season|MinVeryNegative|\n",
      "+------+---------------+\n",
      "|   S08|            181|\n",
      "+------+---------------+\n",
      "\n",
      "+------+-----------+\n",
      "|Season|MaxNegative|\n",
      "+------+-----------+\n",
      "|   S02|        416|\n",
      "+------+-----------+\n",
      "\n",
      "+------+-----------+\n",
      "|Season|MinNegative|\n",
      "+------+-----------+\n",
      "|   S08|        137|\n",
      "+------+-----------+\n",
      "\n",
      "+------+----------+\n",
      "|Season|MaxNeutral|\n",
      "+------+----------+\n",
      "|   S02|      1689|\n",
      "+------+----------+\n",
      "\n",
      "+------+----------+\n",
      "|Season|MinNeutral|\n",
      "+------+----------+\n",
      "|   S08|       741|\n",
      "+------+----------+\n",
      "\n",
      "+------+-----------+\n",
      "|Season|MaxPositive|\n",
      "+------+-----------+\n",
      "|   S02|        434|\n",
      "+------+-----------+\n",
      "\n",
      "+------+-----------+\n",
      "|Season|MinPositive|\n",
      "+------+-----------+\n",
      "|   S08|        159|\n",
      "+------+-----------+\n",
      "\n",
      "+------+---------------+\n",
      "|Season|MaxVeryPositive|\n",
      "+------+---------------+\n",
      "|   S02|            772|\n",
      "+------+---------------+\n",
      "\n",
      "+------+---------------+\n",
      "|Season|MinVeryPositive|\n",
      "+------+---------------+\n",
      "|   S08|            248|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_distribution.createOrReplaceTempView(\"sentiment_counts\")\n",
    "\n",
    "sentiment_max_min = \"\"\"\n",
    "    select max(VeryNegative),\n",
    "            min(VeryNegative),\n",
    "            max(Negative),\n",
    "            min(Negative),\n",
    "            max(Neutral),\n",
    "            min(Neutral),\n",
    "            max(Positive),\n",
    "            min(Positive),\n",
    "            max(VeryPositive),\n",
    "            min(VeryPositive)\n",
    "    from sentiment_counts\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sentiment_max_min).collect()[0]\n",
    "print(f\"{result}\\n\\n\")\n",
    "\n",
    "list_sentiments = ['VeryNegative','Negative','Neutral','Positive','VeryPositive']\n",
    "counter = 0\n",
    "\n",
    "for sentiment in list_sentiments:\n",
    "    query = f\"select Season,{sentiment} as Max{sentiment} from sentiment_counts where {sentiment} = {result[0+counter]}\"\n",
    "    spark.sql(query).show()\n",
    "    counter+=1\n",
    "    \n",
    "    query = f\"select Season,{sentiment} as Min{sentiment} from sentiment_counts where {sentiment} = {result[0+counter]}\"\n",
    "    spark.sql(query).show()\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Season: string, VeryNegative: bigint, Negative: bigint, Neutral: bigint, Positive: bigint, VeryPositive: bigint]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_distribution.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now changing our perspective to the main characters, how'd be their distribution of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------+-------+--------+------------+\n",
      "|       CharacterName|VeryNegative|Negative|Neutral|Positive|VeryPositive|\n",
      "+--------------------+------------+--------+-------+--------+------------+\n",
      "|    Cersei Lannister|         184|     107|    382|     107|         225|\n",
      "|     Jaime Lannister|         160|      89|    414|     124|         158|\n",
      "|    Tyrion Lannister|         302|     201|    609|     215|         433|\n",
      "|          Arya Stark|         106|      73|    448|      71|          85|\n",
      "|Brandon \"Bran\" Stark|          52|      33|    213|      48|          54|\n",
      "|            Jon Snow|         153|     117|    571|     113|         180|\n",
      "|         Sansa Stark|         118|      72|    345|      91|         158|\n",
      "|  Daenerys Targaryen|         134|     116|    465|     128|         206|\n",
      "|       Samwell Tarly|          67|      51|    254|      78|         109|\n",
      "+--------------------+------------+--------+-------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_split_query = \"\"\"\n",
    "    select CharacterName,\n",
    "            sum(VeryNegative) as VeryNegative,\n",
    "            sum(Negative) as Negative,\n",
    "            sum(Neutral) as Neutral,\n",
    "            sum(Positive) as Positive,\n",
    "            sum(VeryPositive) as VeryPositive\n",
    "    from (\n",
    "        select CharacterName,House,\n",
    "                if(SentimentResult='Very Negative',1,0) as VeryNegative,\n",
    "                if(SentimentResult='Negative',1,0) as Negative,\n",
    "                if(SentimentResult='Neutral',1,0) as Neutral,\n",
    "                if(SentimentResult='Positive',1,0) as Positive,\n",
    "                if(SentimentResult='Very Positive',1,0) as VeryPositive\n",
    "        from sentiment_analysis\n",
    "        where CharacterName = 'Jon Snow' or CharacterName = 'Arya Stark' or\n",
    "                CharacterName = 'Sansa Stark' or CharacterName = 'Brandon \"Bran\" Stark' or\n",
    "                CharacterName = 'Cersei Lannister' or CharacterName = 'Jaime Lannister' or\n",
    "                CharacterName = 'Tyrion Lannister' or CharacterName = 'Samwell Tarly' or\n",
    "                CharacterName = 'Daenerys Targaryen'\n",
    "        )\n",
    "    group by CharacterName,House\n",
    "    order by House,CharacterName asc\n",
    "\"\"\"\n",
    "\n",
    "sentiment_distribution = spark.sql(sentiment_split_query)\n",
    "sentiment_distribution.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
